{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import optax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sampling_utils import get_samples, get_z_samples\n",
    "from reparameterization import get_rp_gradients, objective\n",
    "from likelihoodratio import get_lr_gradients\n",
    "from constants import RAND_KEY\n",
    "from training import fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_0 = 1.\n",
    "V_0 = 0.0001\n",
    "\n",
    "A = 1.5\n",
    "B = 1.\n",
    "\n",
    "C = 1.\n",
    "E = .000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs, xs = get_samples(100, 1000, mu_0, V_0, A, B, C, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(zs.mean(axis=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(xs[:-1], xs[1:], color=\"purple\", label=\"x\")\n",
    "plt.scatter(zs[:-1], zs[1:], color=\"orange\", label=\"z\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal A using gradient estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to maximise the marginal likelihood $\\max_{A} \\mathbb{E}_{p(Z|A, B, \\mu_0, V_0)} [p(X|Z, C, E)]$ w.r.t $A$, by using LR and RP gradient estimators together with an Adam optimiser until convergence.\n",
    "\n",
    "We assume here all other parameters ($\\mu_0, V_0, B, C, E$) are known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "num_inputs = 2\n",
    "NUM_TRAIN_STEPS = 3000\n",
    "A = 1.5\n",
    "\n",
    "# Set to False for RP gradient estimator\n",
    "LR_ESTIMATOR = True \n",
    "\n",
    "A_n = 1.\n",
    "_, xs = get_samples(num_inputs, N, mu_0, V_0, A, B, C, E)\n",
    "\n",
    "init_param = jnp.array([A_n])\n",
    "\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    optax.adam(learning_rate=0.0005),\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "optimizer.init(init_param)\n",
    "\n",
    "learned_params, losses, gradients = fit(\n",
    "    params=init_param,\n",
    "    optimizer=optimizer, \n",
    "    training_steps=NUM_TRAIN_STEPS, \n",
    "    mu0=mu_0, V0=V_0, B=B, C=C, E=E, xs=xs, \n",
    "    num_inputs=num_inputs, \n",
    "    N=N, \n",
    "    lr_estimator=LR_ESTIMATOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "As = np.linspace(-4, 4, 100)\n",
    "\n",
    "_, xs = get_samples(num_inputs, N, mu_0, V_0, A, B, C, E)\n",
    "lr_grads = np.zeros(100,)\n",
    "rp_grads = np.zeros(100,)\n",
    "obj = np.zeros(100,)\n",
    "key = RAND_KEY\n",
    "for i in range(100):\n",
    "    a = As[i]\n",
    "    key, subkey = jrandom.split(key)\n",
    "    zs, epsilons = get_z_samples(num_inputs, N, mu_0, V_0, a, B, subkey)\n",
    "    lr_grads[i] = get_lr_gradients(mu_0, V_0, a, B, C, E, xs, zs)\n",
    "    rp_grads[i] = get_rp_gradients(mu_0, V_0, a, B, C, E, epsilons, xs)\n",
    "    obj[i] = objective(mu_0, V_0, a, B, C, E, epsilons, xs)\n",
    "\n",
    "print(f\"Max marginal likelihood reached by RP gradients: {obj[np.argmin(list(map(lambda x: abs(x - 0), rp_grads)))]:.4f}\")\n",
    "print(f\"Max marginal likelihood reached by LR gradients: {obj[np.argmin(list(map(lambda x: abs(x - 0), lr_grads)))]:.4f}\")\n",
    "print(f\"Marginal likelihood of true A: {max(obj):.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(As, obj, color=\"green\", label=\"Marginal likelihood\")\n",
    "plt.plot(As, lr_grads, color=\"purple\", label=\"LR grads\")\n",
    "plt.plot(As, rp_grads, color=\"cornflowerblue\", label=\"RP gradients\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDS produced with learned parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_A = learned_params[0]\n",
    "zs, xs = get_samples(num_inputs, N, mu_0, V_0, learned_A, B, C, E)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(xs[:-1], xs[1:], color=\"purple\", label=\"x\")\n",
    "plt.scatter(zs[:-1], zs[1:], color=\"orange\", label=\"z\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0d0f0160cacfb35fe8642b675725525dd1061e27bd6ae6db2781150554066f0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
